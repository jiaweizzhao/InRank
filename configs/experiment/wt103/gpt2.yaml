# @package _global_

# to execute this experiment run:
# python run.py experiment=example_simple.yaml

defaults:
  - override /trainer: default # choose trainer from 'configs/trainer/'
  - override /model: gpt2-hf
  - override /datamodule: wt103-gpt2
  - override /optimizer: adamw
  - override /scheduler: linear-warmup
  - override /callbacks: default
  - override /metrics: [perplexity]
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

task:
  _target_: src.tasks.seq.SequenceLMModel

seed: 1113

trainer:
  accelerator: gpu
  devices: 8
  num_nodes: 1
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices} * ${datamodule.batch_size} * ${trainer.num_nodes}}}
  max_epochs: 100
  precision: 16
  gradient_clip_val: 1.0
  strategy: ddp

datamodule:
  batch_size: 8  # Per GPU
  max_length: 1024

train:
  global_batch_size: 512
  optimizer:
    lr: 6e-4
    weight_decay: 0.1
  optimizer_param_grouping:
    bias_weight_decay: False
    normalization_weight_decay: False
  scheduler:
    num_warmup_steps: ${div_up:${datamodule.__train_len}, ${train.global_batch_size}}
    num_training_steps: ${eval:${div_up:${datamodule.__train_len}, ${train.global_batch_size}} * ${trainer.max_epochs}}

callbacks:
  model_checkpoint:
    monitor: val/loss
    mode: min
    save_top_k: -1
  early_stopping:
    monitor: val/loss
    mode: min

name: gpt2-baseline
